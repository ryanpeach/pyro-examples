{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81568354",
   "metadata": {},
   "source": [
    "# Mathematical Nomenclature\n",
    "\n",
    "https://en.wikipedia.org/wiki/Variational_Bayesian_methods\n",
    "\n",
    "- `x` - Observations. `pyro.sample` with the `obs` argument\n",
    "- `z` - latent random variables, which I have so far determined are distributions which are sampleable from to get random numbers. Latent means that the variables value can only be infered indirectly. These are the `pyro.sample` values in the code without an `obs` parameter. I believe that any `z` is an actual number, a value of the latent variable randomly sampled, rather than an abstract concept. These are basically outputs in any other model.\n",
    "- $\\theta$ - parameters. These are fixed numbers. `pyro.param`\n",
    "- `posterior` - The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes' rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de2839",
   "metadata": {},
   "source": [
    "# Model Learning\n",
    "\n",
    "The first equation seems to mean that the maximum parameters are those that maximize the probability returned by them when asked to predict the probability of the observations. Which makes sense.\n",
    "\n",
    "The second equation tells us that the removal of either observations `x` or latent variables `z` from the function $p_\\theta$ implies that one must integrate over the missing argument. Which also makes sense. In this case we integrate over all possible values of the latent variable. It then points out this is intractable. I'm unsure if integrating over a latent variable is weighted by the frequency of any value.\n",
    "\n",
    "It then tells us to call the probability of the observations given the parameters is best called \"evidence\", and the log evidence is what is to be maximized. Log probabilities are useful for a lot of reasons best described here\n",
    "\n",
    "https://en.wikipedia.org/wiki/Log_probability\n",
    "\n",
    "The last equation in this section gives us a posterior formula for any value of z given any value of x. It says that the probability given the parameters of a certain value of the latent variables, given a certain observation, is equal to the probability given the parameters of the observation and the latent variable value co-ocuring, over the integration over all values of the latent variable and the single observation. This makes sense, it's basically saying that the probability of the latent variable value given an observation is it's frequentist likelihood of co-occuring with the observation over the frequentist liklihood of that observation occuring with any latent variable value, which is just a ratio of it happening together vs it happening at all. This is only under the condition of the max parameterization.\n",
    "\n",
    "The reason we would want this posterior is so we can know something about the unobserved latent variables, which is the real topic-of-interest for the model. These are basically outputs in any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443a2ce",
   "metadata": {},
   "source": [
    "# Guide\n",
    "\n",
    "So we don't know $p_\\theta_{max}$ but we want to so we can know posteriors. So how will we find it? Well we will approximate it. Just for formalization, the approximation will need a new name, call it a guide and call it $q_\\phi$. Call $\\phi$ variational parameters because we will be varrying them as we learn. Call Q the variational distribution. Q(Z) will mimic P(Z|X), **although I don't know how this is possible with the X removed...** For now I'll just accept it.\n",
    "\n",
    "The guide does not contain observations. It is the pure truth, what would the probability of z be if we didn't see anything. We need this because this is like what we do when we integrate over z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9c7f9",
   "metadata": {},
   "source": [
    "# ELBO\n",
    "\n",
    "Evidence Lower BOund tells us that the expected value of the guide distribution, for a parameterization, for a latent variable, is the log probability of the observation and the latent variable co-occuring minus the log probability of the guide's estimate of the latent variable occuring independently. A minus in log space is like a division, so its similar to saying the log of the probability of the observation and the latent variable co-occuring over the approximate probability of the latent variable occuring, which is very similar to the original posterior equation just in log space.\n",
    "\n",
    "ELBO calculates the lower bound of the log evidence, not the log evidence itself. But it's calculatable, because the model is calculatable and the guide is calculatable, and all you need to do is maximize it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
